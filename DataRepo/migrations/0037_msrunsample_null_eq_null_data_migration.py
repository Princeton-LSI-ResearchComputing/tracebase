# Generated by Django 4.2.11 on 2024-04-16 23:09

from collections import defaultdict
from django.db import migrations, models, transaction


@transaction.atomic
def handle_msrunsample_uniqueconstraint_violations(apps, _):
    """This method will create temporary fake ms_data_file records for those MSRunSample records that violate the new
    unique constraint that is applied to placeholder records (those without a linked ms_data_file).  Note that the
    intention is for the study owner to decide which of each pair of records to delete.  Once that deletion is decided
    upon in each case, another migration (or manual database update) will be necessary to remove one of each pair and
    remove the fake ms_data_file link from the other (and remove the orphaned ArchiveFile record).  Also note that the
    associated accucor files will need to be updated."""

    MSRunSample = apps.get_model("DataRepo", "MSRunSample")
    ArchiveFile = apps.get_model("DataRepo", "ArchiveFile")
    DataType = apps.get_model("DataRepo", "DataType")
    DataFormat = apps.get_model("DataRepo", "DataFormat")

    # If a pre-existing DataType is not found, create the default
    try:
        mz_dt = DataType.objects.get(code="ms_data")
    except DataType.DoesNotExist:
        mz_dt = DataType.objects.create(
            code="ms_data",
            name="Mass Spectrometry Data",
            description="Peak data from a mass spectrometry run",
        )

    # If a pre-existing DataFormat is not found, create the default
    try:
        mz_df = DataFormat.objects.get(code="mzxml")
    except DataFormat.DoesNotExist:
        mz_df = DataFormat.objects.create(
            code="mzxml", name="mzXML", description="mzXML format peak data from a mass spectrometry run"
        )

    # Collect every MSRunSample placeholder record by sequence & sample (the new unique constraint)
    by_uc = defaultdict(list)
    for msrs in MSRunSample.objects.filter(ms_data_file__isnull=True):
        by_uc[f"{msrs.msrun_sequence} {msrs.sample}"].append(msrs)

    uc_violations = [msrs_group for msrs_group in by_uc.values() if len(msrs_group) > 1]

    for uc_violation_group in uc_violations:
        for idx, msrs_in_violation in enumerate(uc_violation_group):
            # Create the fake archive file record
            temp_fake_mzfile = ArchiveFile.objects.create(
                filename=(
                    # Including the researcher in the filename, so we know who must resolve the duplicate
                    f"temp_{msrs_in_violation.sample}_SeqID{msrs_in_violation.msrun_sequence.id}_"
                    f"{msrs_in_violation.msrun_sequence.researcher}_dupe{idx}.mzXML"
                ),
                checksum=f"temp_{msrs_in_violation.sample.id}_SeqID{msrs_in_violation.msrun_sequence.id}_dupe{idx}",
                data_type=mz_dt,
                data_format=mz_df,
            )
            temp_fake_mzfile.full_clean()

            # Update the MSRunSequence record to link to the fake ArchiveFile record
            msrs_in_violation.ms_data_file = temp_fake_mzfile
            msrs_in_violation.full_clean()
            msrs_in_violation.save(update_fields=["ms_data_file"])


class Migration(migrations.Migration):

    dependencies = [
        ("DataRepo", "0036_update_instruments"),
    ]

    operations = [
        migrations.RunPython(handle_msrunsample_uniqueconstraint_violations),
    ]
