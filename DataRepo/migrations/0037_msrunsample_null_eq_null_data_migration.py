# Generated by Django 4.2.11 on 2024-04-16 23:09

from collections import defaultdict
from django.db import migrations


def handle_msrunsample_uniqueconstraint_violations(apps, _):
    """This method will merge placeholder records, re-link the peak group records, and remove polarity, mz_min, and
    mz_max annotations in all placeholders.

    If the re-linking of peakgroups to a common placeholder causes a unique constraint violation, this method will
    create temporary fake ms_data_file records for those MSRunSample records that violate the new unique constraint
    that is applied to placeholder records (those without a linked ms_data_file).

    Note that the intention is for the study owner to decide which of each pair of records to delete.  Once that
    deletion is decided upon in each case, another migration (or manual database update) will be necessary to remove one
    of each pair and remove the fake ms_data_file link from the other (and remove the orphaned ArchiveFile record).
    Also note that the associated accucor files will need to be updated."""

    MSRunSample = apps.get_model("DataRepo", "MSRunSample")
    PeakGroup = apps.get_model("DataRepo", "PeakGroup")
    ArchiveFile = apps.get_model("DataRepo", "ArchiveFile")
    DataType = apps.get_model("DataRepo", "DataType")
    DataFormat = apps.get_model("DataRepo", "DataFormat")

    # If a pre-existing DataType is not found, create the default
    try:
        mz_dt = DataType.objects.get(code="ms_data")
    except DataType.DoesNotExist:
        mz_dt = DataType.objects.create(
            code="ms_data",
            name="Mass Spectrometry Data",
            description="Peak data from a mass spectrometry run",
        )

    # If a pre-existing DataFormat is not found, create the default
    try:
        mz_df = DataFormat.objects.get(code="mzxml")
    except DataFormat.DoesNotExist:
        mz_df = DataFormat.objects.create(
            code="mzxml", name="mzXML", description="mzXML format peak data from a mass spectrometry run"
        )

    # Collect every MSRunSample placeholder record by sequence & sample (the new unique constraint)
    by_uc = defaultdict(list)
    for msrs in MSRunSample.objects.filter(ms_data_file__isnull=True):
        by_uc[f"{msrs.msrun_sequence.pk} {msrs.sample.pk}"].append(msrs)

    uc_violations = [msrs_group for msrs_group in by_uc.values() if len(msrs_group) > 1]

    # Of those violating the unique constraint, those with no duplicate peak groups can be pre-emptively merged and the
    # one that would have 2 peakgroups for the same compound(/peak group name) if merged would need fake ms_data_file
    # records until the researcher can decide between the peak groups as to which to keep and which to throw out before
    # merging.  Once the records have been updated, this migration can be applied again to merge the records.
    msrs_merge_groups = []
    num_to_merge = 0
    msrs_fake_groups = []
    num_to_fake = 0

    # Divide the MSRunSample records that violate the unique constraint into the 2 groups: needing merge and needing a
    # fake ArchiveFile record
    for uc_violation_group in uc_violations:
        pg_names = []
        dupe_pgs = []
        for msrs_in_violation in uc_violation_group:
            current_pg_names = msrs_in_violation.peak_groups.values_list("name", flat=True)
            for name in current_pg_names:
                if name not in pg_names:
                    pg_names.append(name)
                elif name not in dupe_pgs:
                    dupe_pgs.append(name)
        if len(dupe_pgs) > 0:
            msrs_fake_groups.append({"group": uc_violation_group, "dupe_pgs": dupe_pgs})
            num_to_fake += len(uc_violation_group)
        else:
            msrs_merge_groups.append(uc_violation_group)
            num_to_merge += len(uc_violation_group)

    # For the groups that need fake ArchiveFile records
    for uc_violation_group_dict in msrs_fake_groups:
        uc_violation_group = uc_violation_group_dict["group"]
        dupe_pg_str = ",".join(uc_violation_group_dict["dupe_pgs"])

        for idx, msrs_in_violation in enumerate(uc_violation_group):
            # Create the fake archive file record
            temp_fake_mzfile = ArchiveFile.objects.create(
                filename=(
                    # Including the researcher in the filename, so we know who must resolve the duplicate
                    f"Sample{msrs_in_violation.sample}_Sequence{msrs_in_violation.msrun_sequence.id}_"
                    f"{msrs_in_violation.msrun_sequence.researcher}_Dupe{idx}_PeakGroupsToAddress-{dupe_pg_str}.mzXML"
                ),
                checksum=f"Sample{msrs_in_violation.sample.id}_Sequence{msrs_in_violation.msrun_sequence.id}_Dupe{idx}",
                data_type=mz_dt,
                data_format=mz_df,
            )
            temp_fake_mzfile.full_clean()

            # Update the MSRunSequence record to link to the fake ArchiveFile record
            msrs_in_violation.ms_data_file = temp_fake_mzfile
            msrs_in_violation.full_clean()
            msrs_in_violation.save(update_fields=["ms_data_file"])

    # For the groups that need to be merged
    for uc_violation_group in msrs_merge_groups:
        first_msrs = None

        for idx, msrs_in_violation in enumerate(uc_violation_group):

            if idx == 0:

                # Set the first one aside.  We're going to update it after the deletions of the others, just in case it
                # runs afoul of the current unique constraint (though I don't think it will given null != null, but it
                # doesn't hurt either)
                first_msrs = msrs_in_violation

            else:

                pgs = list(PeakGroup.objects.filter(msrun_sample=msrs_in_violation))
                for pg in pgs:
                    pg.msrun_sample = first_msrs
                    pg.full_clean()
                    pg.save(update_fields=["msrun_sample"])

                # After having re-linked the Peak Groups, delete the newly empty MSRunSample placeholder
                msrs_in_violation.delete()

        if first_msrs is not None and (
            first_msrs.polarity is not None
            or first_msrs.mz_min is not None
            or first_msrs.mz_max is not None
        ):
            # Null-out the annotations because they may be inaccurate WRT the peak groups
            first_msrs.polarity = None
            first_msrs.mz_min = None
            first_msrs.mz_max = None
            first_msrs.full_clean()
            first_msrs.save(update_fields=["polarity", "mz_min", "mz_max"])

    # Remove polarity, mz_min, and mz_max from unique placeholder records, as we don't know if the peak groups that link
    # to them all came from mzXML files with the same scan range and/or polarity.
    single_annotated_placeholders = [
        msrs_group[0]
        for msrs_group in by_uc.values()
        if len(msrs_group) == 1 and (
            msrs_group[0].polarity is not None
            or msrs_group[0].mz_min is not None
            or msrs_group[0].mz_max is not None
        )
    ]

    for placeholder in single_annotated_placeholders:
        placeholder.polarity = None
        placeholder.mz_min = None
        placeholder.mz_max = None
        placeholder.full_clean()
        placeholder.save()

    print(f"\n{num_to_merge} MSRunSample records merged down to {len(msrs_merge_groups)} records edited to remove "
        "polarity, mz_min, and/or mz_max.")
    print(
        f"{num_to_fake} MSRunSample records given fake mzXML files to resolve unique constraint because they have "
        "multiple peak groups with the same compound."
    )
    print(
        f"{len(single_annotated_placeholders)} MSRunSample single placeholder records edited to remove polarity, "
        "mz_min, and/or mz_max."
    )


class Migration(migrations.Migration):

    dependencies = [
        ("DataRepo", "0036_update_instruments"),
    ]

    operations = [
        migrations.RunPython(handle_msrunsample_uniqueconstraint_violations),
    ]
